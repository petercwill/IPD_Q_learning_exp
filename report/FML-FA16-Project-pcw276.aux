\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{axelrod1987evolution}
\@writefile{toc}{\contentsline {section}{\tocsection {}{1}{Motivation}}{1}{section.1}}
\citation{fogel1993evolving}
\citation{sandholm1996multiagent}
\@writefile{toc}{\contentsline {section}{\tocsection {}{2}{Preliminaries}}{2}{section.2}}
\newlabel{sec:preliminaries}{{2}{2}{Preliminaries}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{3}{Q-learning Implementation}}{3}{section.3}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{4}{Experiment 1: Q-learning against Tit-for-Tat}}{4}{section.4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table showing convergence counts to steady cooperation (C), steady defection (D), and alternating Cooperation and Delectations (M) for IPD games of 10,000 trials }}{5}{table.1}}
\newlabel{table:conv}{{1}{5}{Table showing convergence counts to steady cooperation (C), steady defection (D), and alternating Cooperation and Delectations (M) for IPD games of 10,000 trials}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{5}{Experiment 2: Q-learner against Q-learner}}{5}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Last 40,000 iterations of IPD play until 1000 (R,R) game streak threshold between a 4th-order Q-Learner with discount rate set to .9, learning rate set to .2 and annealing constants a=20, b=.999, against tit-for-tat player }}{6}{figure.1}}
\newlabel{fig:heat_conv}{{1}{6}{Last 40,000 iterations of IPD play until 1000 (R,R) game streak threshold between a 4th-order Q-Learner with discount rate set to .9, learning rate set to .2 and annealing constants a=20, b=.999, against tit-for-tat player}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Average score for first 20,000 iterations of an IPD game, for two Q-Learners with identical parameters (order = 3, $\alpha $ = .2, $\gamma $=.9, a=50, b=.999). Blue dashed line shows the mean value taken across 10 IPD games, yellow shaded region denotes $\pm $ one standard deviation}}{7}{figure.2}}
\newlabel{fig:equal}{{2}{7}{Average score for first 20,000 iterations of an IPD game, for two Q-Learners with identical parameters (order = 3, $\alpha $ = .2, $\gamma $=.9, a=50, b=.999). Blue dashed line shows the mean value taken across 10 IPD games, yellow shaded region denotes $\pm $ one standard deviation}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{6}{Experiment 3: Tournament}}{7}{section.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces 10,000 iterations of IPD play for two Q-learners with identical parameters (order = 3, $\alpha $ = .2, $\gamma $=.9, a=50, b=.999). Shown from Player 1's perspective. Convergence to steady-state {T,S,P,P,P} cycle shown. }}{8}{figure.3}}
\newlabel{fig:heat_equal}{{3}{8}{10,000 iterations of IPD play for two Q-learners with identical parameters (order = 3, $\alpha $ = .2, $\gamma $=.9, a=50, b=.999). Shown from Player 1's perspective. Convergence to steady-state {T,S,P,P,P} cycle shown}{figure.3}{}}
\bibstyle{abbrv}
\bibdata{FML-FA16-Project-pcw276}
\bibcite{axelrod1987evolution}{{1}{}{{}}{{}}}
\bibcite{fogel1993evolving}{{2}{}{{}}{{}}}
\bibcite{sandholm1996multiagent}{{3}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Average score for first 20,000 iterations of an IPD game, for two Q-Learners with different parameters. {\bf  Player 1} with: (order = 5, $\alpha $ = .2, $\gamma $=.9, a=50, b=.999). {\bf  Player 2} with: (order = 2, $\alpha $ = .2, $\gamma $=.9, a=50, b=.8) Blue dashed line shows the mean value taken across 10 IPD games, yellow shaded region denotes $\pm $ one standard deviation}}{9}{figure.4}}
\newlabel{fig:unequal}{{4}{9}{Average score for first 20,000 iterations of an IPD game, for two Q-Learners with different parameters. {\bf Player 1} with: (order = 5, $\alpha $ = .2, $\gamma $=.9, a=50, b=.999). {\bf Player 2} with: (order = 2, $\alpha $ = .2, $\gamma $=.9, a=50, b=.8) Blue dashed line shows the mean value taken across 10 IPD games, yellow shaded region denotes $\pm $ one standard deviation}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\tocsection {}{}{References}}{9}{section*.1}}
\newlabel{tocindent-1}{0pt}
\newlabel{tocindent0}{15.0pt}
\newlabel{tocindent1}{21.0pt}
\newlabel{tocindent2}{0pt}
\newlabel{tocindent3}{0pt}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces 10,000 iterations of IPD play for two Q-learners with different parameters {\bf  Player 1} with: (order = 5, $\alpha $ = .2, $\gamma $=.9, a=50, b=.999). {\bf  Player 2} with: (order = 2, $\alpha $ = .2, $\gamma $=.9, a=50, b=.8). Shown from Player 1's perspective. Convergence to steady-state {T,P} cycle shown. }}{10}{figure.5}}
\newlabel{fig:heat_unequal}{{5}{10}{10,000 iterations of IPD play for two Q-learners with different parameters {\bf Player 1} with: (order = 5, $\alpha $ = .2, $\gamma $=.9, a=50, b=.999). {\bf Player 2} with: (order = 2, $\alpha $ = .2, $\gamma $=.9, a=50, b=.8). Shown from Player 1's perspective. Convergence to steady-state {T,P} cycle shown}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Results for 10 independent IDP tournaments shown. For each tournament, every player plays every other player in 10,000 iterations. Heatmap colorbar shows position that the strategy finished the tournament in (blue denoting high-place finishers, red low-place finishers)}}{11}{figure.6}}
\newlabel{fig:tournament}{{6}{11}{Results for 10 independent IDP tournaments shown. For each tournament, every player plays every other player in 10,000 iterations. Heatmap colorbar shows position that the strategy finished the tournament in (blue denoting high-place finishers, red low-place finishers)}{figure.6}{}}
